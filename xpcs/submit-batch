#!/usr/bin/env python
import argparse
import glob
import getpass
import json
import time
import subprocess
import os
import sys
import shutil
import pexpect
from add_dag import add_dag
UNAME = getpass.getuser()

def check_existing_tunnel(thetahost, thetaport):
    '''return true if there's already a matching SSH tunnel'''
    ps_cmd = f'ps aux | grep {UNAME} | grep "ssh -f -N -L {thetaport}" | grep -v grep'
    ps_check = subprocess.run(
         ps_cmd,
         stdout=subprocess.PIPE,
         stderr=subprocess.STDOUT,
         shell=True,
         encoding="utf-8",
    )
    if thetahost in ps_check.stdout:
        print("Detected already-running SSH tunnel:\n", ps_check.stdout.strip())
        return True
    else:
        return False

def connect(thetahost, thetaport):
    '''Set up SSH tunnel with port-forwarding, if there isn't already one alive'''
    if check_existing_tunnel(thetahost, thetaport):
        return

    credential = getpass.getpass("Please authenticate to Theta: ")
    ssh_cmd = f"ssh -f -N -L {thetaport}:localhost:{thetaport} {UNAME}@{thetahost}.alcf.anl.gov"
    ssh_cmd = f"ssh -f -N -L {thetaport}:localhost:{thetaport} -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null {UNAME}@{thetahost}.alcf.anl.gov"
    tunnel = pexpect.spawn(ssh_cmd)

    tunnel.expect('Password:')
    time.sleep(0.1)
    tunnel.sendline(credential)
    time.sleep(0.1)
    return tunnel

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--db', required=True, help="path to directory containing server-info")
    parser.add_argument('--argfile', required=True, help="path to file containing list of .hdf/.imm paths (one pair per line)")
    parser.add_argument('--result-dir', required=True, help="Where to stage back results")
    args = parser.parse_args()
    args.db = os.path.abspath(os.path.expanduser(args.db))
    args.argfile = os.path.abspath(os.path.expanduser(args.argfile))
    args.result_dir = os.path.abspath(os.path.expanduser(args.result_dir))
    return args

def validate_args(args):
    assert os.path.isdir(args.db)
    assert os.path.isfile(args.argfile)
    info_path = os.path.join(args.db, 'server-info')
    assert os.path.isdir(args.result_dir)

def read_server_info(db_path):
    info_path = os.path.join(db_path, 'server-info')
    with open(info_path) as fp: info = json.load(fp)
    thetaport = info["port"]
    thetahost = info.get("thetahost", info["host"])
    info["thetahost"] = thetahost
    info["host"] = "localhost"
    with open(info_path, 'w') as fp: fp.write(json.dumps(info))
    return thetahost, thetaport

def check_apps(required_apps):
    '''Ensure that requisite XPCS apps are already registered'''
    from balsam.core.models import ApplicationDefinition as App
    found_apps = [app.name for app in App.objects.all()]
    assert all(app in found_apps for app in required_apps)
    print(f"Got all required apps ({required_apps}): OK!")

def main():
    args = get_args()
    validate_args(args)

    os.environ['BALSAM_DB_PATH'] = os.path.abspath(args.db)
    thetahost, thetaport = read_server_info(args.db)
    tunnel = connect(thetahost, thetaport)

    check_apps(["corr",])
    from balsam.core.models import BalsamJob

    # create DAG
    jobs = []
    with open(args.argfile) as fp:
        for line in fp:
            if line.startswith('VALID:'):
                _, h5, imm = line.split()
                assert h5.endswith('.hdf')
                assert os.path.isfile(h5)
                assert imm.endswith('.imm')
                assert os.path.isfile(imm)

                # We cannot globus transfer from /gdata, so we first stage locally
                h5_stage = os.path.join(args.result_dir, h5[1:])
                imm_stage = os.path.join(args.result_dir, imm[1:])
                if not os.path.isdir(os.path.dirname(h5_stage)):
                    os.makedirs(os.path.dirname(h5_stage))
                if not os.path.isdir(os.path.dirname(imm_stage)):
                    os.makedirs(os.path.dirname(imm_stage))
                shutil.copy(h5, h5_stage)
                shutil.copy(imm, imm_stage)

                job = add_dag(h5_stage, imm_stage, args.result_dir)
                jobs.append(job)
    BalsamJob.objects.bulk_create(jobs)
    print(f"Added {len(jobs)} new jobs to DB")

if __name__ == "__main__":
    main()
